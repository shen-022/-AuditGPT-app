import logging
import warnings
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, LabelEncoder

# é…ç½®è­¦å‘Šå’Œæ—¥å¿—
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


# --- é…ç½®å’Œå¸¸é‡ ---
class Config:
    """é…ç½®ç±»"""
    PAGE_TITLE = "æ™ºèƒ½å®¡è®¡AIå¹³å°"
    APP_NAME = "æ™ºé‰´AuditGPT"
    VERSION = "v2.0"
    MAX_FILE_SIZE = 200  # MB
    SUPPORTED_FORMATS = ['csv', 'xlsx', 'xls']
    DEFAULT_CONTAMINATION = 0.02
    RANDOM_STATE = 42


# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title=Config.PAGE_TITLE,
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- è‡ªå®šä¹‰CSSæ ·å¼ ---
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #667eea;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    .anomaly-card {
        background: #ffebee;
        border-left: 4px solid #f44336;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .normal-card {
        background: #e8f5e8;
        border-left: 4px solid #4caf50;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .stButton > button {
        width: 100%;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.5rem 2rem;
        border-radius: 25px;
        font-weight: bold;
        transition: all 0.3s;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(102, 126, 234, 0.3);
    }
</style>
""", unsafe_allow_html=True)

# --- æ ‡é¢˜éƒ¨åˆ† ---
st.markdown(f"""
<div class="main-header">
    <h1>ğŸ” {Config.APP_NAME}</h1>
    <p>åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½å®¡è®¡å¼‚å¸¸æ£€æµ‹å¹³å° {Config.VERSION}</p>
</div>
""", unsafe_allow_html=True)


# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    default_states = {
        'df': None,
        'processed_df': None,
        'anomalies_df': None,
        'model': None,
        'scaler': None,
        'X_scaled': None,
        'label_encoders': {},
        'feature_names': [],
        'analysis_complete': False,
        'file_name': None
    }

    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value


initialize_session_state()


# --- æ•°æ®å¤„ç†å‡½æ•° ---
class DataProcessor:
    """æ•°æ®å¤„ç†ç±»"""

    @staticmethod
    def detect_data_types(df: pd.DataFrame) -> Dict[str, List[str]]:
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        datetime_cols = []

        # å°è¯•æ£€æµ‹æ—¥æœŸæ—¶é—´åˆ—
        for col in categorical_cols:
            sample = df[col].dropna().head(100)
            try:
                pd.to_datetime(sample)
                datetime_cols.append(col)
            except:
                pass

        # ä»åˆ†ç±»åˆ—ä¸­ç§»é™¤æ—¥æœŸæ—¶é—´åˆ—
        categorical_cols = [col for col in categorical_cols if col not in datetime_cols]

        return {
            'numeric': numeric_cols,
            'categorical': categorical_cols,
            'datetime': datetime_cols
        }

    @staticmethod
    def preprocess_data(df: pd.DataFrame, target_features: List[str] = None) -> Tuple[pd.DataFrame, Dict]:
        """æ•°æ®é¢„å¤„ç†"""
        processed_df = df.copy()
        encoders = {}

        # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾ç±»å‹
        data_types = DataProcessor.detect_data_types(processed_df)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç‰¹å¾ï¼Œè‡ªåŠ¨é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        if target_features is None:
            target_features = data_types['numeric']

        # å¤„ç†åˆ†ç±»å˜é‡
        for col in data_types['categorical']:
            if col in target_features:
                le = LabelEncoder()
                processed_df[col] = le.fit_transform(processed_df[col].astype(str))
                encoders[col] = le

        # å¤„ç†æ—¥æœŸæ—¶é—´å˜é‡
        for col in data_types['datetime']:
            if col in target_features:
                processed_df[col] = pd.to_datetime(processed_df[col])
                # æå–æœ‰ç”¨çš„æ—¶é—´ç‰¹å¾
                processed_df[f'{col}_hour'] = processed_df[col].dt.hour
                processed_df[f'{col}_dayofweek'] = processed_df[col].dt.dayofweek
                processed_df[f'{col}_month'] = processed_df[col].dt.month

                # æ›´æ–°ç›®æ ‡ç‰¹å¾åˆ—è¡¨
                target_features.extend([f'{col}_hour', f'{col}_dayofweek', f'{col}_month'])
                if col in target_features:
                    target_features.remove(col)

        # å¤„ç†ç¼ºå¤±å€¼
        processed_df = processed_df.fillna(processed_df.mean(numeric_only=True))
        processed_df = processed_df.fillna('Unknown')

        return processed_df, encoders


class ModelAnalyzer:
    """æ¨¡å‹åˆ†æç±»"""

    @staticmethod
    def train_isolation_forest(X: pd.DataFrame, contamination: float = Config.DEFAULT_CONTAMINATION) -> Tuple[
        IsolationForest, StandardScaler]:
        """è®­ç»ƒIsolation Forestæ¨¡å‹"""
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # è®­ç»ƒæ¨¡å‹
        model = IsolationForest(
            contamination=contamination,
            random_state=Config.RANDOM_STATE,
            n_estimators=100,
            max_samples='auto',
            n_jobs=-1
        )
        model.fit(X_scaled)

        return model, scaler

    @staticmethod
    def generate_advanced_report(anomaly_score: float, feature_names: List[str],
                                 sample_data: pd.Series) -> Tuple[List[str], List[Dict]]:
        """ç”Ÿæˆé«˜çº§è§£é‡ŠæŠ¥å‘Šï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨SHAPï¼‰"""
        report = []

        # å¼‚å¸¸å¾—åˆ†è§£é‡Š
        risk_level = "é«˜é£é™©" if anomaly_score < -0.5 else "ä¸­é£é™©" if anomaly_score < -0.2 else "ä½é£é™©"
        risk_color = "ğŸ”´" if risk_level == "é«˜é£é™©" else "ğŸŸ¡" if risk_level == "ä¸­é£é™©" else "ğŸŸ¢"

        report.append("### ğŸ“Š å¼‚å¸¸é£é™©è¯„ä¼°")
        report.append(f"{risk_color} **é£é™©ç­‰çº§**: {risk_level}")
        report.append(f"**å¼‚å¸¸å¾—åˆ†**: {anomaly_score:.4f} (è¶Šä½è¶Šå¼‚å¸¸)")
        report.append("")

        # ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        feature_impacts = []

        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ç‰¹å¾å€¼åˆ†æä½œä¸ºæ›¿ä»£
        for feature in feature_names:
            if feature in sample_data.index:
                value = sample_data[feature]
                # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼šæç«¯å€¼å¯èƒ½æ›´é‡è¦
                if isinstance(value, (int, float)):
                    # å‡è®¾æ•°å€¼å‹ç‰¹å¾çš„æç«¯å€¼æ›´å¯èƒ½å¼‚å¸¸
                    impact = abs(value - sample_data[feature].mean()) / sample_data[feature].std() if sample_data[
                                                                                                          feature].std() > 0 else 0
                    direction = "æ¨å¼‚å¸¸" if abs(value) > 2 else "æ¨æ­£å¸¸"  # å‡è®¾ç»å¯¹å€¼å¤§äº2æ ‡å‡†å·®ä¸ºå¼‚å¸¸
                    color = "ğŸ”´" if direction == "æ¨å¼‚å¸¸" else "ğŸ”µ"

                    feature_impacts.append({
                        'ç‰¹å¾åç§°': feature,
                        'ç‰¹å¾å€¼': round(value, 4),
                        'å½±å“æ–¹å‘': direction,
                        'å½±å“ç¨‹åº¦': "é«˜" if impact > 2 else "ä¸­" if impact > 1 else "ä½",
                        'SHAPå€¼': round(impact, 4),
                        'é‡è¦æ€§æ’å': 0,
                        'æ ‡å¿—': color
                    })

        # æŒ‰å½±å“å€¼æ’åºå¹¶åˆ†é…æ’å
        feature_impacts.sort(key=lambda x: x['SHAPå€¼'], reverse=True)
        for i, impact in enumerate(feature_impacts):
            impact['é‡è¦æ€§æ’å'] = i + 1

        # ç”Ÿæˆå…³é”®å‘ç°
        report.append("### ğŸ¯ å…³é”®å‘ç°")

        anomaly_drivers = [f for f in feature_impacts if f['å½±å“æ–¹å‘'] == 'æ¨å¼‚å¸¸'][:3]
        normal_drivers = [f for f in feature_impacts if f['å½±å“æ–¹å‘'] == 'æ¨æ­£å¸¸'][:3]

        if anomaly_drivers:
            report.append("**ğŸ” ä¸»è¦å¼‚å¸¸é©±åŠ¨å› ç´ ï¼š**")
            for i, driver in enumerate(anomaly_drivers, 1):
                report.append(f"{i}. **{driver['ç‰¹å¾åç§°']}** = {driver['ç‰¹å¾å€¼']}")
                report.append(f"   - å½±å“æ–¹å‘: {driver['æ ‡å¿—']} {driver['å½±å“æ–¹å‘']}")
                report.append(f"   - å½±å“ç¨‹åº¦: {driver['å½±å“ç¨‹åº¦']}")
            report.append("")

        if normal_drivers:
            report.append("**âœ… ä¸»è¦æ­£å¸¸é©±åŠ¨å› ç´ ï¼š**")
            for i, driver in enumerate(normal_drivers, 1):
                report.append(f"{i}. **{driver['ç‰¹å¾åç§°']}** = {driver['ç‰¹å¾å€¼']}")
                report.append(f"   - å½±å“æ–¹å‘: {driver['æ ‡å¿—']} {driver['å½±å“æ–¹å‘']}")
                report.append(f"   - å½±å“ç¨‹åº¦: {driver['å½±å“ç¨‹åº¦']}")
            report.append("")

        # å®¡è®¡å»ºè®®
        report.append("### ğŸ’¡ æ™ºèƒ½å®¡è®¡å»ºè®®")
        if risk_level == "é«˜é£é™©":
            report.append("ğŸš¨ **ç´§æ€¥å…³æ³¨**")
            report.append("1. ç«‹å³è¿›è¡Œè¯¦ç»†å®¡æŸ¥")
            report.append("2. æ ¸å®æ‰€æœ‰ç›¸å…³å•æ®å’Œå‡­è¯")
            report.append("3. è”ç³»ç›¸å…³ä¸šåŠ¡äººå‘˜ç¡®è®¤")
            if anomaly_drivers:
                report.append(f"4. é‡ç‚¹æ£€æŸ¥ **{anomaly_drivers[0]['ç‰¹å¾åç§°']}** ç›¸å…³ä¸šåŠ¡")
        elif risk_level == "ä¸­é£é™©":
            report.append("âš ï¸ **é‡ç‚¹å…³æ³¨**")
            report.append("1. è¿›è¡ŒæŠ½æ ·å¤æ ¸")
            report.append("2. ä¸å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”")
            report.append("3. å¿…è¦æ—¶è¿›è¡Œè¿›ä¸€æ­¥è°ƒæŸ¥")
        else:
            report.append("â„¹ï¸ **å¸¸è§„å¤„ç†**")
            report.append("1. æŒ‰å¸¸è§„å®¡è®¡ç¨‹åºå¤„ç†")
            report.append("2. å¯ä½œä¸ºå¯¹æ¯”åŸºå‡†")

        return report, feature_impacts


# --- ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.markdown("### âš™ï¸ æ¨¡å‹å‚æ•°é…ç½®")

    contamination = st.slider(
        "å¼‚å¸¸æ¯”ä¾‹ (%)",
        min_value=0.1,
        max_value=10.0,
        value=2.0,
        step=0.1,
        help="é¢„æœŸæ•°æ®ä¸­å¼‚å¸¸æ ·æœ¬çš„æ¯”ä¾‹"
    ) / 100

    st.markdown("### ğŸ“ˆ åˆ†æé€‰é¡¹")
    show_data_profile = st.checkbox("æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ", value=True)
    show_correlation = st.checkbox("æ˜¾ç¤ºç‰¹å¾ç›¸å…³æ€§", value=True)
    auto_feature_selection = st.checkbox("è‡ªåŠ¨ç‰¹å¾é€‰æ‹©", value=True)

# --- ä¸»è¦å†…å®¹åŒºåŸŸ ---
st.header("ğŸ“¤ 1. æ•°æ®ä¸Šä¼ ä¸é¢„è§ˆ")

# æ–‡ä»¶ä¸Šä¼ 
uploaded_file = st.file_uploader(
    "è¯·ä¸Šä¼ å®¡è®¡æ•°æ®æ–‡ä»¶",
    type=Config.SUPPORTED_FORMATS,
    help=f"æ”¯æŒæ ¼å¼: {', '.join(Config.SUPPORTED_FORMATS)}ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°: {Config.MAX_FILE_SIZE}MB"
)

if uploaded_file is not None:
    try:
        # è¯»å–æ–‡ä»¶
        if st.session_state.file_name != uploaded_file.name:
            with st.spinner('æ­£åœ¨è¯»å–æ–‡ä»¶...'):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)

                st.session_state.df = df
                st.session_state.file_name = uploaded_file.name
                st.session_state.analysis_complete = False

            st.success(f"âœ… æ–‡ä»¶ '{uploaded_file.name}' ä¸Šä¼ æˆåŠŸï¼")

        df = st.session_state.df

        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»è®°å½•æ•°", len(df))
        with col2:
            st.metric("ç‰¹å¾æ•°é‡", len(df.columns))
        with col3:
            st.metric("ç¼ºå¤±å€¼", df.isnull().sum().sum())
        with col4:
            st.metric("é‡å¤è¡Œ", df.duplicated().sum())

        # æ•°æ®é¢„è§ˆ
        st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
        preview_rows = st.selectbox("æ˜¾ç¤ºè¡Œæ•°", [5, 10, 20, 50], index=0)
        st.dataframe(df.head(preview_rows), use_container_width=True)

        # æ•°æ®æ¦‚è§ˆ
        if show_data_profile:
            with st.expander("ğŸ“Š æ•°æ®æ¦‚è§ˆåˆ†æ", expanded=False):
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("æ•°æ®ç±»å‹åˆ†å¸ƒ")
                    data_types = DataProcessor.detect_data_types(df)
                    type_counts = {k: len(v) for k, v in data_types.items()}

                    fig = px.pie(
                        values=list(type_counts.values()),
                        names=list(type_counts.keys()),
                        title="ç‰¹å¾ç±»å‹åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    st.subheader("ç¼ºå¤±å€¼ç»Ÿè®¡")
                    missing_data = df.isnull().sum()
                    missing_data = missing_data[missing_data > 0]

                    if not missing_data.empty:
                        fig = px.bar(
                            x=missing_data.index,
                            y=missing_data.values,
                            title="å„ç‰¹å¾ç¼ºå¤±å€¼æ•°é‡"
                        )
                        fig.update_xaxis(tickangle=45)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("ğŸ‰ æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±å€¼ï¼")

        # ç‰¹å¾é€‰æ‹©
        st.markdown("---")
        st.header("ğŸ¯ 2. ç‰¹å¾é€‰æ‹©ä¸é…ç½®")

        if auto_feature_selection:
            # è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
            data_types = DataProcessor.detect_data_types(df)
            suggested_features = data_types['numeric']

            # æ·»åŠ ä¸€äº›å¯èƒ½çš„åˆ†ç±»ç‰¹å¾
            categorical_features = [col for col in data_types['categorical']
                                    if df[col].nunique() < 20]  # é™åˆ¶åˆ†ç±»æ•°é‡
            suggested_features.extend(categorical_features[:3])  # æœ€å¤šæ·»åŠ 3ä¸ªåˆ†ç±»ç‰¹å¾

            st.info(f"ğŸ¤– è‡ªåŠ¨æ¨èç‰¹å¾: {', '.join(suggested_features)}")
        else:
            suggested_features = []

        # æ‰‹åŠ¨ç‰¹å¾é€‰æ‹©
        available_columns = df.columns.tolist()
        selected_features = st.multiselect(
            "é€‰æ‹©ç”¨äºå¼‚å¸¸æ£€æµ‹çš„ç‰¹å¾",
            available_columns,
            default=suggested_features,
            help="é€‰æ‹©æ•°å€¼å‹ç‰¹å¾æ•ˆæœæ›´å¥½ï¼Œåˆ†ç±»ç‰¹å¾ä¼šè‡ªåŠ¨ç¼–ç "
        )

        if len(selected_features) < 2:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©2ä¸ªç‰¹å¾è¿›è¡Œå¼‚å¸¸æ£€æµ‹")
        else:
            # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
            if show_correlation and len(selected_features) > 2:
                with st.expander("ğŸ“ˆ ç‰¹å¾ç›¸å…³æ€§åˆ†æ", expanded=False):
                    numeric_features = df[selected_features].select_dtypes(include=[np.number]).columns
                    if len(numeric_features) > 1:
                        corr_matrix = df[numeric_features].corr()

                        fig = px.imshow(
                            corr_matrix,
                            title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾",
                            color_continuous_scale="RdBu_r",
                            aspect="auto"
                        )
                        st.plotly_chart(fig, use_container_width=True)

                        # é«˜ç›¸å…³æ€§æé†’
                        high_corr = np.where(np.abs(corr_matrix) > 0.8)
                        high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y])
                                           for x, y in zip(*high_corr) if x != y]
                        if high_corr_pairs:
                            st.warning(f"âš ï¸ å‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹: {high_corr_pairs[:3]}")

            # æ¨¡å‹è®­ç»ƒ
            st.markdown("---")
            st.header("ğŸ¤– 3. å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ")

            if st.button("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†æ", type="primary"):
                with st.spinner('ğŸ”„ AIæ¨¡å‹æ­£åœ¨è¿›è¡Œå¼‚å¸¸æ£€æµ‹åˆ†æ...'):
                    try:
                        # æ•°æ®é¢„å¤„ç†
                        processed_df, encoders = DataProcessor.preprocess_data(df, selected_features)

                        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
                        feature_cols = []
                        for feature in selected_features:
                            if feature in processed_df.columns:
                                feature_cols.append(feature)
                            else:
                                # æŸ¥æ‰¾å¯èƒ½çš„æ´¾ç”Ÿç‰¹å¾ï¼ˆå¦‚æ—¶é—´ç‰¹å¾ï¼‰
                                derived = [col for col in processed_df.columns if col.startswith(f'{feature}_')]
                                feature_cols.extend(derived)

                        X = processed_df[feature_cols]

                        # è®­ç»ƒæ¨¡å‹
                        model, scaler = ModelAnalyzer.train_isolation_forest(X, contamination)

                        # é¢„æµ‹
                        X_scaled = scaler.transform(X)
                        predictions = model.predict(X_scaled)
                        anomaly_scores = model.decision_function(X_scaled)

                        # ä¿å­˜ç»“æœ
                        processed_df['å¼‚å¸¸æ ‡è¯†'] = predictions
                        processed_df['å¼‚å¸¸å¾—åˆ†'] = anomaly_scores
                        processed_df['å¼‚å¸¸åˆ¤å®š'] = processed_df['å¼‚å¸¸æ ‡è¯†'].apply(
                            lambda x: "å¼‚å¸¸" if x == -1 else "æ­£å¸¸")

                        # æ›´æ–°ä¼šè¯çŠ¶æ€
                        st.session_state.processed_df = processed_df
                        st.session_state.anomalies_df = processed_df[processed_df['å¼‚å¸¸åˆ¤å®š'] == 'å¼‚å¸¸'].copy()
                        st.session_state.model = model
                        st.session_state.scaler = scaler
                        st.session_state.X_scaled = X_scaled
                        st.session_state.label_encoders = encoders
                        st.session_state.feature_names = feature_cols
                        st.session_state.analysis_complete = True

                        st.success("âœ… åˆ†æå®Œæˆï¼")
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                        st.info("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")

    except Exception as e:
        st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")

# --- ç»“æœå±•ç¤º ---
if st.session_state.analysis_complete and st.session_state.anomalies_df is not None:
    st.markdown("---")
    st.header("ğŸ“Š 4. å¼‚å¸¸æ£€æµ‹ç»“æœ")

    anomalies_df = st.session_state.anomalies_df
    total_records = len(st.session_state.processed_df)
    anomaly_count = len(anomalies_df)
    anomaly_rate = (anomaly_count / total_records) * 100

    # ç»“æœç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»è®°å½•æ•°", total_records)
    with col2:
        st.metric("å¼‚å¸¸è®°å½•æ•°", anomaly_count, delta=f"{anomaly_count}")
    with col3:
        st.metric("å¼‚å¸¸æ¯”ä¾‹", f"{anomaly_rate:.2f}%")
    with col4:
        avg_score = anomalies_df['å¼‚å¸¸å¾—åˆ†'].mean()
        st.metric("å¹³å‡å¼‚å¸¸å¾—åˆ†", f"{avg_score:.3f}")

    # å¼‚å¸¸è®°å½•å±•ç¤º
    st.subheader("ğŸ“‹ å¼‚å¸¸äº¤æ˜“åˆ—è¡¨")
    st.dataframe(anomalies_df.head(20), use_container_width=True)

    if len(anomalies_df) > 20:
        st.info(f"å…±å‘ç° {len(anomalies_df)} æ¡å¼‚å¸¸è®°å½•ï¼Œä»…æ˜¾ç¤ºå‰20æ¡")

    # å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒ
    st.subheader("ğŸ“ˆ å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒ")
    fig = px.histogram(
        st.session_state.processed_df,
        x='å¼‚å¸¸å¾—åˆ†',
        color='å¼‚å¸¸åˆ¤å®š',
        title='å¼‚å¸¸å¾—åˆ†åˆ†å¸ƒ',
        nbins=50,
        color_discrete_map={'å¼‚å¸¸': 'red', 'æ­£å¸¸': 'green'}
    )
    st.plotly_chart(fig, use_container_width=True)

    # è¯¦ç»†åˆ†æ
    st.markdown("---")
    st.header("ğŸ” 5. è¯¦ç»†åˆ†æ")

    # é€‰æ‹©è¦åˆ†æçš„å¼‚å¸¸è®°å½•
    if not anomalies_df.empty:
        selected_index = st.selectbox(
            "é€‰æ‹©è¦è¯¦ç»†åˆ†æçš„å¼‚å¸¸äº¤æ˜“",
            anomalies_df.index,
            format_func=lambda x: f"è®°å½• {x} (å¾—åˆ†: {anomalies_df.loc[x, 'å¼‚å¸¸å¾—åˆ†']:.3f})"
        )

        if selected_index:
            selected_anomaly = anomalies_df.loc[selected_index]
            st.subheader(f"ğŸ“ å¼‚å¸¸äº¤æ˜“è¯¦æƒ… - è®°å½• {selected_index}")

            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**äº¤æ˜“ç‰¹å¾å€¼:**")
                for feature in st.session_state.feature_names:
                    if feature in selected_anomaly:
                        st.write(f"- {feature}: {selected_anomaly[feature]}")

            with col2:
                st.markdown("**å¼‚å¸¸ä¿¡æ¯:**")
                st.write(f"- å¼‚å¸¸å¾—åˆ†: {selected_anomaly['å¼‚å¸¸å¾—åˆ†']:.4f}")
                st.write(f"- å¼‚å¸¸æ ‡è¯†: {selected_anomaly['å¼‚å¸¸åˆ¤å®š']}")

            # ç”Ÿæˆè§£é‡ŠæŠ¥å‘Š
            if st.button("ğŸ“Š ç”Ÿæˆè¯¦ç»†è§£é‡ŠæŠ¥å‘Š"):
                with st.spinner('æ­£åœ¨ç”Ÿæˆè§£é‡ŠæŠ¥å‘Š...'):
                    report, feature_impacts = ModelAnalyzer.generate_advanced_report(
                        selected_anomaly['å¼‚å¸¸å¾—åˆ†'],
                        st.session_state.feature_names,
                        selected_anomaly
                    )

                    # æ˜¾ç¤ºæŠ¥å‘Š
                    st.markdown("---")
                    st.header("ğŸ“‹ å¼‚å¸¸äº¤æ˜“è§£é‡ŠæŠ¥å‘Š")

                    for line in report:
                        st.markdown(line)

                    # æ˜¾ç¤ºç‰¹å¾å½±å“è¡¨æ ¼
                    st.subheader("ğŸ“ˆ ç‰¹å¾å½±å“æ±‡æ€»è¡¨")
                    impact_df = pd.DataFrame(feature_impacts)
                    if not impact_df.empty:
                        st.dataframe(impact_df[['ç‰¹å¾åç§°', 'ç‰¹å¾å€¼', 'å½±å“æ–¹å‘', 'å½±å“ç¨‹åº¦', 'SHAPå€¼', 'é‡è¦æ€§æ’å']])
                    else:
                        st.info("æ— æ³•ç”Ÿæˆè¯¦ç»†çš„ç‰¹å¾å½±å“åˆ†æ")

# å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œæ˜¾ç¤ºæŒ‡å¼•
else:
    st.info("ğŸ‘† è¯·ä¸Šä¼ å®¡è®¡æ•°æ®æ–‡ä»¶å¼€å§‹åˆ†æ")


    # ç¤ºä¾‹æ•°æ®ä¸‹è½½
    @st.cache_data
    def generate_sample_data():
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        n_samples = 1000

        # æ­£å¸¸æ•°æ®
        normal_data = {
            'äº¤æ˜“é‡‘é¢': np.random.normal(1000, 300, n_samples),
            'äº¤æ˜“æ—¶é—´é—´éš”': np.random.exponential(5, n_samples),
            'è´¦æˆ·å†å²äº¤æ˜“ç¬”æ•°': np.random.poisson(50, n_samples),
            'äº¤æ˜“ç±»å‹': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),
            'å•†æˆ·ç±»åˆ«': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2])
        }

        # å¼‚å¸¸æ•°æ®
        n_anomalies = 20
        anomaly_data = {
            'äº¤æ˜“é‡‘é¢': np.concatenate([
                np.random.uniform(5000, 10000, n_anomalies // 2),
                np.random.uniform(10, 50, n_anomalies // 2)
            ]),
            'äº¤æ˜“æ—¶é—´é—´éš”': np.random.uniform(0.1, 1, n_anomalies),
            'è´¦æˆ·å†å²äº¤æ˜“ç¬”æ•°': np.random.randint(1, 5, n_anomalies),
            'äº¤æ˜“ç±»å‹': np.random.choice([0, 1], n_anomalies, p=[0.8, 0.2]),
            'å•†æˆ·ç±»åˆ«': np.random.choice(['A', 'D'], n_anomalies, p=[0.3, 0.7])
        }

        # åˆå¹¶æ•°æ®
        df_normal = pd.DataFrame(normal_data)
        df_anomaly = pd.DataFrame(anomaly_data)

        return pd.concat([df_normal, df_anomaly], ignore_index=True)


    sample_df = generate_sample_data()
    csv = sample_df.to_csv(index=False).encode('utf-8')

    st.download_button(
        label="ğŸ“¥ ä¸‹è½½ç¤ºä¾‹æ•°æ® (sample_audit_data.csv)",
        data=csv,
        file_name='sample_audit_data.csv',
        mime='text/csv',
        help="ç‚¹å‡»ä¸‹è½½ç¤ºä¾‹å®¡è®¡æ•°æ®è¿›è¡Œæµ‹è¯•"
    )